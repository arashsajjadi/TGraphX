{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CIFAR‑10 Image Classification: Graph‑based vs. CNN Baseline\n",
    "\n",
    "In this notebook we compare two approaches for CIFAR‑10 image classification:\n",
    "1. **Graph‑based Model:** Each 32×32 CIFAR‑10 image is divided into 16 patches (a 4×4 grid, each patch is 8×8). These patches become node features, and adjacent patches are connected to form a graph. We then process the graph using our unified CNN‑GNN model (which integrates a modern CNN encoder with dropout, batch normalization, and residual connections into a GNN).\n",
    "2. **CNN Baseline:** A conventional convolutional neural network for CIFAR‑10 classification.\n",
    "\n",
    "We train both models and compare their learning curves and performance.\n"
   ],
   "id": "8fc08d95715b06c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T10:32:53.100183Z",
     "start_time": "2025-03-22T10:32:50.533962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import libraries and set up device\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# TGraphX imports\n",
    "from models.cnn_gnn_model import CNN_GNN_Model\n",
    "from core.graph import Graph, GraphBatch\n",
    "from core.dataloader import GraphDataset, GraphDataLoader\n",
    "from core.utils import get_device\n",
    "\n",
    "device = get_device()\n",
    "print(\"Using device:\", device)\n"
   ],
   "id": "63877e4b54f908bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Custom Dataset: CIFAR‑10 Graph Dataset\n",
    "\n",
    "We define a dataset that loads CIFAR‑10 images and converts each image into a graph using the helper above.\n"
   ],
   "id": "859c234219d5563d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T10:32:53.131110Z",
     "start_time": "2025-03-22T10:32:53.114182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "class CIFAR10GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, grid_size=4):\n",
    "        self.dataset = CIFAR10(root=root, train=train, download=True, transform=transform)\n",
    "        self.grid_size = grid_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]  # img is a tensor, label is an int\n",
    "        # Convert image (3, 32, 32) into graph (node_features and edge_index)\n",
    "        node_features, edge_index = image_to_graph(img, grid_size=self.grid_size)\n",
    "        from core.graph import Graph\n",
    "        graph = Graph(node_features, edge_index)\n",
    "        return graph, label\n",
    "\n",
    "# Define transform to convert PIL image to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Optionally add normalization\n",
    "])\n"
   ],
   "id": "f9905fefd222e5ac",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DataLoader Setup\n",
    "\n",
    "We use the custom `GraphDataLoader` from TGraphX with a collate function that batches (graph, label) tuples.\n"
   ],
   "id": "d7662f843f6b7794"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T10:32:54.323819Z",
     "start_time": "2025-03-22T10:32:53.239819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def graph_label_collate_fn(batch):\n",
    "    \"\"\"Batch a list of (Graph, label) tuples.\"\"\"\n",
    "    graphs, labels = zip(*batch)\n",
    "    from core.graph import GraphBatch\n",
    "    return GraphBatch(graphs), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "train_dataset = CIFAR10GraphDataset(root='./data', train=True, transform=transform, grid_size=4)\n",
    "test_dataset = CIFAR10GraphDataset(root='./data', train=False, transform=transform, grid_size=4)\n",
    "\n",
    "from core.dataloader import GraphDataLoader\n",
    "train_loader = GraphDataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=graph_label_collate_fn)\n",
    "test_loader  = GraphDataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=graph_label_collate_fn)\n"
   ],
   "id": "ada0b3a9b07c9bed",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Helper Functions: Converting an Image to a Graph\n",
    "\n",
    "Each CIFAR‑10 image (3×32×32) is split into a 4×4 grid of patches (each 3×8×8).\n",
    "Edges are created between patches that are adjacent (right and bottom neighbors).\n"
   ],
   "id": "2b2800a60ad5bbb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T10:32:54.355189Z",
     "start_time": "2025-03-22T10:32:54.340822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def image_to_graph(img, grid_size=4, extra_edge_prob=0.1):\n",
    "    \"\"\"\n",
    "    Converts an image tensor (C, H, W) into a graph.\n",
    "    Divides the image into grid_size x grid_size patches.\n",
    "    Creates structured edges between adjacent patches and adds extra random edges.\n",
    "\n",
    "    Returns:\n",
    "        node_features: Tensor of shape (grid_size*grid_size, C, patch_H, patch_W)\n",
    "        edge_index: Tensor of shape [2, E]\n",
    "    \"\"\"\n",
    "    C, H, W = img.shape\n",
    "    patch_H = H // grid_size\n",
    "    patch_W = W // grid_size\n",
    "    nodes = []\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            patch = img[:, i*patch_H:(i+1)*patch_H, j*patch_W:(j+1)*patch_W]\n",
    "            nodes.append(patch)\n",
    "    node_features = torch.stack(nodes, dim=0)\n",
    "\n",
    "    edges = []\n",
    "    def node_index(i, j):\n",
    "        return i * grid_size + j\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            idx = node_index(i, j)\n",
    "            if j < grid_size - 1:\n",
    "                edges.append([idx, node_index(i, j+1)])\n",
    "                edges.append([node_index(i, j+1), idx])\n",
    "            if i < grid_size - 1:\n",
    "                edges.append([idx, node_index(i+1, j)])\n",
    "                edges.append([node_index(i+1, j), idx])\n",
    "    # Extra random edges\n",
    "    total_nodes = grid_size * grid_size\n",
    "    for i in range(total_nodes):\n",
    "        for j in range(i+1, total_nodes):\n",
    "            if random.random() < extra_edge_prob:\n",
    "                edges.append([i, j])\n",
    "                edges.append([j, i])\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    return node_features, edge_index\n",
    "\n",
    "# Test the helper with a sample image from the dataset\n",
    "sample_graph, sample_label = train_dataset[0]\n",
    "print(\"Graph Sample Summary:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Label: {sample_label}\")\n",
    "print(f\"Node features shape: {sample_graph.node_features.shape}\")\n",
    "print(f\"Edge index shape: {sample_graph.edge_index.shape}\")\n",
    "print(\"First 10 edges:\")\n",
    "print(sample_graph.edge_index[:, :10])\n",
    "print(\"-\" * 40)\n"
   ],
   "id": "85868faf9f46bd48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Sample Summary:\n",
      "----------------------------------------\n",
      "Label: 6\n",
      "Node features shape: torch.Size([16, 3, 8, 8])\n",
      "Edge index shape: torch.Size([2, 70])\n",
      "First 10 edges:\n",
      "tensor([[0, 1, 0, 4, 1, 2, 1, 5, 2, 3],\n",
      "        [1, 0, 4, 0, 2, 1, 5, 1, 3, 2]])\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model 1: Graph-based Classifier using TGraphX\n",
    "\n",
    "Our graph-based model uses node features from image patches. For each graph:\n",
    "- **Input shape:** (3, 8, 8) per node.\n",
    "- **Hidden shape:** We choose (16, 8, 8).\n",
    "- **Pooling:** Mean pooling over nodes.\n",
    "- **Classifier:** Outputs 10 classes.\n"
   ],
   "id": "8e0cd82a62df0c9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T10:32:54.541200Z",
     "start_time": "2025-03-22T10:32:54.434546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sample configuration dictionary\n",
    "config = {\n",
    "    \"cnn_params\": {\n",
    "         \"in_channels\": 3,              # Number of input channels for the CNN encoder (e.g. 3 for RGB)\n",
    "         \"out_features\": 64,            # Number of output channels for the CNN encoder (feature map channels)\n",
    "         \"num_layers\": 2,               # Total number of convolutional layers in the CNN encoder\n",
    "         \"hidden_channels\": 64,         # Number of channels in the intermediate layers of the CNN encoder\n",
    "         \"dropout_prob\": 0.3,           # Dropout probability used in the CNN encoder\n",
    "         \"use_batchnorm\": True,         # Flag to use Batch Normalization in the CNN encoder\n",
    "         \"use_residual\": True,          # Flag to enable residual (skip) connections in the CNN encoder\n",
    "         \"pool_layers\": 2,              # Number of early layers on which to apply max pooling\n",
    "         \"debug\": False,                # Debug flag to print internal shapes and stats during CNN encoding\n",
    "         \"return_feature_map\": True     # When True, the CNN encoder returns a spatial feature map (not flattened)\n",
    "    },\n",
    "    \"use_preencoder\": False,              # Flag to enable the optional pre-encoder stage\n",
    "    \"pretrained_resnet\": False,           # Flag to choose whether to load pretrained ResNet-18 weights for the pre-encoder\n",
    "    \"preencoder_params\": {\n",
    "         \"in_channels\": 3,             # Number of input channels for the pre-encoder (e.g. 3 for RGB)\n",
    "         \"out_channels\": 32,           # Number of output channels from the pre-encoder\n",
    "         \"hidden_channels\": 32         # Number of channels in the custom pre-encoder (if not using pretrained)\n",
    "    },\n",
    "    \"gnn_in_dim\": (64, 5, 5),            # Input dimensions for the GNN layers (must match CNN encoder output)\n",
    "    \"gnn_hidden_dim\": (128, 5, 5),         # Hidden state dimensions for the GNN layers (multi-dimensional)\n",
    "    \"num_classes\": 10,                   # Number of classes for classification (e.g. 10 for CIFAR-10)\n",
    "    \"num_gnn_layers\": 4,                 # Total number of GNN message passing layers\n",
    "    \"gnn_dropout\": 0.3,                  # Dropout probability to use in the GNN layers\n",
    "    \"residual\": True,                    # Flag to enable residual (skip) connections in the GNN layers\n",
    "    \"aggregator_params\": {               # Additional parameters for the deep CNN aggregator used in the GNN layers\n",
    "         \"num_layers\": 4,               # Number of convolutional layers in the aggregator CNN\n",
    "         \"dropout_prob\": 0.3,           # Dropout probability in the aggregator CNN\n",
    "         \"use_batchnorm\": True          # Flag to use BatchNorm in the aggregator CNN\n",
    "    }\n",
    "}\n",
    "\n",
    "from models.pre_encoder import PreEncoder  # Import the PreEncoder module\n",
    "from models.cnn_gnn_model import CNN_GNN_Model  # Import the unified CNN-GNN model\n",
    "from core.utils import get_device             # Import helper function to get the available device\n",
    "\n",
    "# Instantiate the pre-encoder if enabled in the configuration.\n",
    "if config[\"use_preencoder\"]:\n",
    "    pre_encoder = PreEncoder(\n",
    "         in_channels=config[\"preencoder_params\"][\"in_channels\"],  # Pass input channels for pre-encoder\n",
    "         out_channels=config[\"preencoder_params\"][\"out_channels\"],# Pass output channels for pre-encoder\n",
    "         use_pretrained=config[\"pretrained_resnet\"],              # Whether to use pretrained ResNet-18 weights\n",
    "         custom_params=config.get(\"preencoder_params\")            # Additional custom parameters (if any)\n",
    "    )\n",
    "else:\n",
    "    pre_encoder = None  # No pre-encoder is used; raw input is directly passed to the CNN encoder.\n",
    "\n",
    "# Create the unified CNN-GNN model using the configuration parameters.\n",
    "model = CNN_GNN_Model(\n",
    "    cnn_params=config[\"cnn_params\"],         # Configuration for the CNN encoder\n",
    "    gnn_in_dim=config[\"gnn_in_dim\"],           # Input dimensions for the GNN layers (matches CNN encoder output)\n",
    "    gnn_hidden_dim=config[\"gnn_hidden_dim\"],   # Hidden dimensions for the GNN layers\n",
    "    num_classes=config[\"num_classes\"],         # Number of classes for classification\n",
    "    num_gnn_layers=config[\"num_gnn_layers\"],   # Total number of GNN layers to stack\n",
    "    gnn_dropout=config[\"gnn_dropout\"],         # Dropout rate for the GNN layers\n",
    "    residual=config[\"residual\"],               # Enable residual connections in the GNN layers\n",
    "    aggregator_params=config[\"aggregator_params\"],  # Parameters for the deep CNN aggregator in GNN layers\n",
    "    pre_encoder=pre_encoder                    # Pass the optional pre-encoder (if instantiated)\n",
    ")\n",
    "\n",
    "# Get the available device (GPU if available, else CPU)\n",
    "device = get_device()\n",
    "\n",
    "# Move the model to the chosen device.\n",
    "model.to(device)\n"
   ],
   "id": "dfd7198eb1dfa7cf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_GNN_Model(\n",
       "  (encoder): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Dropout2d(p=0.3, inplace=False)\n",
       "        (4): SafeMaxPool2d()\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Dropout2d(p=0.3, inplace=False)\n",
       "          (4): SafeMaxPool2d()\n",
       "        )\n",
       "        (proj): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
       "      )\n",
       "    )\n",
       "    (conv1x1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (gnn_layers): ModuleList(\n",
       "    (0): ConvMessagePassing(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (aggregator): DeepCNNAggregator(\n",
       "        (cnn): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Dropout2d(p=0.3, inplace=False)\n",
       "          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (6): ReLU(inplace=True)\n",
       "          (7): Dropout2d(p=0.3, inplace=False)\n",
       "          (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (10): ReLU(inplace=True)\n",
       "          (11): Dropout2d(p=0.3, inplace=False)\n",
       "          (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (14): ReLU(inplace=True)\n",
       "          (15): Dropout2d(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1-3): 3 x ConvMessagePassing(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (aggregator): DeepCNNAggregator(\n",
       "        (cnn): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Dropout2d(p=0.3, inplace=False)\n",
       "          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (6): ReLU(inplace=True)\n",
       "          (7): Dropout2d(p=0.3, inplace=False)\n",
       "          (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (10): ReLU(inplace=True)\n",
       "          (11): Dropout2d(p=0.3, inplace=False)\n",
       "          (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (14): ReLU(inplace=True)\n",
       "          (15): Dropout2d(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model 2: CNN Baseline for CIFAR‑10\n",
    "\n",
    "Below is a simple CNN architecture for CIFAR‑10 classification.\n"
   ],
   "id": "48b4951e0386729c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T10:32:54.572555Z",
     "start_time": "2025-03-22T10:32:54.557555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a simple CNN baseline for CIFAR‑10 classification.\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 32 -> 16\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)   # 16 -> 8\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "cnn_model = SimpleCNN(num_classes=10)\n",
    "cnn_model.to(device)\n"
   ],
   "id": "4162b1d2bd8dd3e0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=4096, out_features=128, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T10:32:54.603585Z",
     "start_time": "2025-03-22T10:32:54.589246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "graph_optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "cnn_optimizer   = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ],
   "id": "a7f952e6d4c23192",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T10:32:55.703020Z",
     "start_time": "2025-03-22T10:32:54.620586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# Create DataLoaders for the CNN baseline using the original CIFAR‑10 images.\n",
    "train_cifar = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_cifar  = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "cnn_train_loader = torch.utils.data.DataLoader(train_cifar, batch_size=32, shuffle=True)\n",
    "cnn_test_loader  = torch.utils.data.DataLoader(test_cifar, batch_size=32, shuffle=False)\n"
   ],
   "id": "2197c81177374acb",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "\n",
    "We train both models on the CIFAR‑10 training set for a few epochs and compare their performance.\n",
    "For the graph-based model, the input graphs are converted from image patches, while the CNN uses the original images.\n"
   ],
   "id": "9e71f1632b6b91a3"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-22T10:32:55.719021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "\n",
    "def evaluate_model(model, loader, device, is_cnn=False):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            if is_cnn:\n",
    "                images, labels = data\n",
    "                images = images.to(device)\n",
    "                outputs = model(images)\n",
    "            else:\n",
    "                graphs, labels = data\n",
    "                graphs = graphs.to(device)\n",
    "                outputs = model(graphs.node_features, graphs.edge_index, graphs.edge_features, graphs.batch)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    ba = balanced_accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return ba, f1\n",
    "\n",
    "num_epochs = 50\n",
    "graph_train_losses = []\n",
    "graph_ba_list = []\n",
    "graph_f1_list = []\n",
    "cnn_train_losses = []\n",
    "cnn_ba_list = []\n",
    "cnn_f1_list = []\n",
    "\n",
    "print(\"\\nTraining Both Models Simultaneously:\\n\")\n",
    "header = \"{:<6s} {:<14s} {:<14s} {:<14s} {:<14s} {:<14s} {:<14s}\".format(\n",
    "    \"Epoch\", \"TGraphX Loss\", \"TGraphX BA\", \"TGraphX F1\", \"CNN Loss\", \"CNN BA\", \"CNN F1\"\n",
    ")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # --- Train Graph-based Model ---\n",
    "    model.train()\n",
    "    total_graph_loss = 0.0\n",
    "    total_graph_samples = 0\n",
    "    graph_nan_detected = False  # flag to mark if NaN is detected in graph training\n",
    "    for batch, labels in train_loader:\n",
    "        # Optionally, you could print the batch shape for the first batch\n",
    "        #if total_graph_samples == 0:\n",
    "        #    print(\"Batch node_features shape:\", batch.node_features.shape)\n",
    "        batch = batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "        graph_optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch.node_features, batch.edge_index, batch.edge_features, batch.batch)\n",
    "        if torch.isnan(outputs).any():\n",
    "            print(\"Model output contains NaN!\")\n",
    "            print(\"Output stats: mean={:.4f}, std={:.4f}, min={:.4f}, max={:.4f}\".format(\n",
    "                outputs.mean().item(), outputs.std().item(), outputs.min().item(), outputs.max().item()\n",
    "            ))\n",
    "            graph_nan_detected = True\n",
    "            break\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        if torch.isnan(loss):\n",
    "            print(\"NaN loss detected in Graph model!\")\n",
    "            graph_nan_detected = True\n",
    "            break\n",
    "\n",
    "        loss.backward()\n",
    "        graph_optimizer.step()\n",
    "        total_graph_loss += loss.item() * labels.size(0)\n",
    "        total_graph_samples += labels.size(0)\n",
    "\n",
    "    if graph_nan_detected or total_graph_samples == 0:\n",
    "        epoch_graph_loss = float('nan')\n",
    "        graph_train_losses.append(epoch_graph_loss)\n",
    "        print(f\"Epoch {epoch}: Graph training encountered NaN values.\")\n",
    "    else:\n",
    "        epoch_graph_loss = total_graph_loss / total_graph_samples\n",
    "        graph_train_losses.append(epoch_graph_loss)\n",
    "\n",
    "    # --- Train CNN Baseline ---\n",
    "    cnn_model.train()\n",
    "    total_cnn_loss = 0.0\n",
    "    total_cnn_samples = 0\n",
    "    for images, labels in cnn_train_loader:\n",
    "        cnn_optimizer.zero_grad()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = cnn_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        cnn_optimizer.step()\n",
    "        total_cnn_loss += loss.item() * labels.size(0)\n",
    "        total_cnn_samples += labels.size(0)\n",
    "    epoch_cnn_loss = total_cnn_loss / total_cnn_samples\n",
    "    cnn_train_losses.append(epoch_cnn_loss)\n",
    "\n",
    "    # --- Evaluate Models ---\n",
    "    graph_ba, graph_f1 = evaluate_model(model, test_loader, device, is_cnn=False)\n",
    "    cnn_ba, cnn_f1 = evaluate_model(cnn_model, cnn_test_loader, device, is_cnn=True)\n",
    "\n",
    "    # Store metrics for plotting\n",
    "    graph_ba_list.append(graph_ba)\n",
    "    graph_f1_list.append(graph_f1)\n",
    "    cnn_ba_list.append(cnn_ba)\n",
    "    cnn_f1_list.append(cnn_f1)\n",
    "\n",
    "    print(\"{:<6d} {:<14.4f} {:<14.4f} {:<14.4f} {:<14.4f} {:<14.4f} {:<14.4f}\".format(\n",
    "        epoch, epoch_graph_loss, graph_ba, graph_f1, epoch_cnn_loss, cnn_ba, cnn_f1\n",
    "    ))\n",
    "\n",
    "print(\"\\nTraining complete!\")\n"
   ],
   "id": "1628862c34711941",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Both Models Simultaneously:\n",
      "\n",
      "Epoch  TGraphX Loss   TGraphX BA     TGraphX F1     CNN Loss       CNN BA         CNN F1        \n",
      "------------------------------------------------------------------------------------------------\n",
      "1      2.0234         0.2480         0.1748         1.7131         0.4936         0.4799        \n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Learning Curve Comparison\n",
    "\n",
    "Below we plot the training loss curves for both models.\n"
   ],
   "id": "909710c5e53eb150"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume these lists are populated during training:\n",
    "# graph_train_losses, graph_ba_list, graph_f1_list,\n",
    "# cnn_train_losses, cnn_ba_list, cnn_f1_list\n",
    "\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5), sharex=True)\n",
    "\n",
    "# Loss plot: Compare TGraphX and CNN losses in the same plot\n",
    "axs[0].plot(epochs, graph_train_losses, color='tab:blue', label='TGraphX Loss')\n",
    "axs[0].plot(epochs, cnn_train_losses, color='tab:orange', label='CNN Loss')\n",
    "axs[0].set_title(\"Loss\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].grid(True)\n",
    "axs[0].legend()\n",
    "\n",
    "# Balanced Accuracy plot: Compare TGraphX and CNN BA in the same plot\n",
    "axs[1].plot(epochs, graph_ba_list, color='tab:blue', label='TGraphX BA')\n",
    "axs[1].plot(epochs, cnn_ba_list, color='tab:orange', label='CNN BA')\n",
    "axs[1].set_title(\"Balanced Accuracy\")\n",
    "axs[1].set_ylabel(\"Balanced Accuracy\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].grid(True)\n",
    "axs[1].legend()\n",
    "\n",
    "# F1 Score plot: Compare TGraphX and CNN F1 in the same plot\n",
    "axs[2].plot(epochs, graph_f1_list, color='tab:blue', label='TGraphX F1')\n",
    "axs[2].plot(epochs, cnn_f1_list, color='tab:orange', label='CNN F1')\n",
    "axs[2].set_title(\"F1 Score\")\n",
    "axs[2].set_ylabel(\"F1 Score\")\n",
    "axs[2].set_xlabel(\"Epoch\")\n",
    "axs[2].grid(True)\n",
    "axs[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "60f1a02e3dbdbfc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "920993d2cb63ad14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
