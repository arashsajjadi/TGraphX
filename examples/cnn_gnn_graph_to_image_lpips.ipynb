{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-22T09:59:05.801084Z",
     "start_time": "2025-03-22T09:59:02.592046Z"
    }
   },
   "source": [
    "# cnn_gnn_graph_to_image_lpips.ipynb\n",
    "\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import lpips  # Make sure to install lpips via: pip install lpips\n",
    "\n",
    "# Enable anomaly detection for better debugging (this may slow down training)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Import our unified CNN-GNN model and necessary utilities\n",
    "from models.cnn_gnn_model import CNN_GNN_Model\n",
    "from core.graph import Graph, GraphBatch\n",
    "from core.dataloader import GraphDataset, GraphDataLoader\n",
    "from core.utils import get_device\n",
    "\n",
    "#############################################\n",
    "#   LPIPS Loss Setup\n",
    "#############################################\n",
    "print(\"Setting up [LPIPS] perceptual loss...\")\n",
    "lpips_criterion = lpips.LPIPS(net='alex')\n",
    "device = get_device()  # obtain device early\n",
    "lpips_criterion = lpips_criterion.to(device)  # move LPIPS model to device\n",
    "\n",
    "def lpips_loss_fn(pred, target):\n",
    "    # Normalize images to [-1, 1] using tanh (assuming inputs are roughly centered)\n",
    "    pred_norm = torch.tanh(pred)\n",
    "    target_norm = torch.tanh(target)\n",
    "    return lpips_criterion(pred_norm, target_norm).mean()\n",
    "\n",
    "#############################################\n",
    "#   Custom Collate Function for (Graph, label) pairs\n",
    "#############################################\n",
    "def graph_label_collate_fn(batch):\n",
    "    \"\"\"Collate a list of (Graph, label) tuples into a batched graph and label tensor.\"\"\"\n",
    "    graphs, labels = zip(*batch)\n",
    "    return GraphBatch(graphs), torch.stack(labels)\n",
    "\n",
    "#############################################\n",
    "#   Synthetic Dataset Generation\n",
    "#############################################\n",
    "# Parameters\n",
    "num_graphs = 50                # total number of graphs\n",
    "num_nodes = 16                 # each graph has 16 nodes (patches)\n",
    "num_edges = 40                 # arbitrary number of edges per graph\n",
    "raw_node_shape = (3, 32, 32)     # each node is a 32x32 RGB patch (raw image)\n",
    "cnn_out_dim = 64              # embedding dimension output by the CNN encoder\n",
    "output_image_shape = (3, 128, 128)  # desired output image: 128x128 RGB\n",
    "num_classes = output_image_shape[0] * output_image_shape[1] * output_image_shape[2]\n",
    "num_epochs = 1000\n",
    "lr = 0.0001  # Lower learning rate to help stabilize training\n",
    "\n",
    "# Additional parameters for dropout, batch norm, and residual connections:\n",
    "cnn_dropout = 0.3    # Dropout probability for CNN layers\n",
    "gnn_dropout = 0.2    # Dropout probability for GNN layers\n",
    "use_batchnorm = True\n",
    "use_residual = True\n",
    "\n",
    "graphs = []\n",
    "labels_list = []\n",
    "for i in range(num_graphs):\n",
    "    node_features = torch.randn(num_nodes, *raw_node_shape)\n",
    "    edge_index = torch.randint(0, num_nodes, (2, num_edges))\n",
    "    graph = Graph(node_features, edge_index)\n",
    "    graphs.append(graph)\n",
    "    target_image = torch.randn(*output_image_shape)\n",
    "    labels_list.append(target_image)\n",
    "\n",
    "# Move graphs and labels to device\n",
    "for graph in graphs:\n",
    "    graph.to(device)\n",
    "labels_list = [lbl.to(device) for lbl in labels_list]\n",
    "\n",
    "# Create dataset and dataloader using our custom collate function\n",
    "dataset = GraphDataset(list(zip(graphs, labels_list)))\n",
    "dataloader = GraphDataLoader(dataset, batch_size=5, shuffle=True, collate_fn=graph_label_collate_fn)\n",
    "\n",
    "#############################################\n",
    "#   Model, Loss, and Optimizer Setup\n",
    "#############################################\n",
    "# Define CNN encoder parameters (with dropout and batch norm)\n",
    "cnn_params = {\n",
    "    'in_channels': raw_node_shape[0],\n",
    "    'out_features': cnn_out_dim,\n",
    "    'num_layers': 4,\n",
    "    'hidden_channels': 32,\n",
    "    'input_spatial_size': raw_node_shape[1],\n",
    "    'dropout_prob': cnn_dropout,      # e.g., 0.3\n",
    "    'use_batchnorm': use_batchnorm\n",
    "}\n",
    "\n",
    "# Instantiate the unified CNN-GNN model with dropout & residuals for the GNN part.\n",
    "# (Assuming your CNN_GNN_Model is updated to accept 'gnn_dropout' and 'residual' parameters)\n",
    "model = CNN_GNN_Model(\n",
    "    cnn_params=cnn_params,\n",
    "    gnn_in_dim=cnn_out_dim,\n",
    "    gnn_hidden_dim=(64,),\n",
    "    num_classes=num_classes,\n",
    "    num_gnn_layers=3,\n",
    "    gnn_dropout=gnn_dropout,  # e.g., 0.2\n",
    "    residual=use_residual     # e.g., True\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#############################################\n",
    "#   Training Loop with Enhanced Debugging\n",
    "#############################################\n",
    "loss_values = []\n",
    "\n",
    "print(\"\\nTraining CNN-GNN model (4-depth CNN encoder with dropout, 3 GNN layers with dropout/residual) using LPIPS loss...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch, batch_labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch.node_features, batch.edge_index, batch.edge_features, batch.batch)\n",
    "\n",
    "        # Debug: check model output statistics before reshaping\n",
    "        with torch.no_grad():\n",
    "            out_mean = logits.mean().item()\n",
    "            out_std = logits.std().item()\n",
    "            out_min = logits.min().item()\n",
    "            out_max = logits.max().item()\n",
    "            # Uncomment below to print these stats for every batch\n",
    "            # print(f\"Pre-reshape: mean={out_mean:.4f}, std={out_std:.4f}, min={out_min:.4f}, max={out_max:.4f}\")\n",
    "\n",
    "        logits = logits.view(-1, *output_image_shape)\n",
    "\n",
    "        # Debug: check image logits stats\n",
    "        with torch.no_grad():\n",
    "            img_mean = logits.mean().item()\n",
    "            img_std = logits.std().item()\n",
    "            img_min = logits.min().item()\n",
    "            img_max = logits.max().item()\n",
    "            # Uncomment below to print these stats for every batch\n",
    "            # print(f\"Post-reshape: mean={img_mean:.4f}, std={img_std:.4f}, min={img_min:.4f}, max={img_max:.4f}\")\n",
    "\n",
    "        loss = lpips_loss_fn(logits, batch_labels)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Detected NaN loss!\")\n",
    "            print(f\"Logits stats: mean={img_mean:.4f}, std={img_std:.4f}, min={img_min}, max={img_max}\")\n",
    "            break\n",
    "\n",
    "        loss.backward()\n",
    "        # Apply gradient clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.9)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * batch_labels.size(0)\n",
    "    epoch_loss /= num_graphs\n",
    "    loss_values.append(epoch_loss)\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {epoch_loss:.4f}\")\n",
    "    if math.isnan(epoch_loss):\n",
    "        print(\"NaN detected in epoch loss. Stopping training for debugging.\")\n",
    "        break\n",
    "\n",
    "#############################################\n",
    "#   Plotting Learning Curve (Loss)\n",
    "#############################################\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(loss_values) + 1), loss_values, linewidth=2)\n",
    "plt.title(\"LPIPS Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"LPIPS Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss...\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arash\\anaconda3\\envs\\tgraphx\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\arash\\anaconda3\\envs\\tgraphx\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: C:\\Users\\arash\\anaconda3\\envs\\tgraphx\\lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'input_spatial_size'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 97\u001B[0m\n\u001B[0;32m     85\u001B[0m cnn_params \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     86\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124min_channels\u001B[39m\u001B[38;5;124m'\u001B[39m: raw_node_shape[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mout_features\u001B[39m\u001B[38;5;124m'\u001B[39m: cnn_out_dim,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     92\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse_batchnorm\u001B[39m\u001B[38;5;124m'\u001B[39m: use_batchnorm\n\u001B[0;32m     93\u001B[0m }\n\u001B[0;32m     95\u001B[0m \u001B[38;5;66;03m# Instantiate the unified CNN-GNN model with dropout & residuals for the GNN part.\u001B[39;00m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;66;03m# (Assuming your CNN_GNN_Model is updated to accept 'gnn_dropout' and 'residual' parameters)\u001B[39;00m\n\u001B[1;32m---> 97\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mCNN_GNN_Model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     98\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcnn_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcnn_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     99\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgnn_in_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcnn_out_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    100\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgnn_hidden_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    101\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_classes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    102\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_gnn_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    103\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgnn_dropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgnn_dropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# e.g., 0.2\u001B[39;49;00m\n\u001B[0;32m    104\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresidual\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_residual\u001B[49m\u001B[43m     \u001B[49m\u001B[38;5;66;43;03m# e.g., True\u001B[39;49;00m\n\u001B[0;32m    105\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m model\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    109\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlr)\n",
      "File \u001B[1;32mD:\\programming\\TGraphX\\models\\cnn_gnn_model.py:21\u001B[0m, in \u001B[0;36mCNN_GNN_Model.__init__\u001B[1;34m(self, cnn_params, gnn_in_dim, gnn_hidden_dim, num_classes, num_gnn_layers, gnn_dropout, residual, aggregator_params, pre_encoder)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Pass the optional pre_encoder to the CNN encoder.\u001B[39;00m\n\u001B[0;32m     20\u001B[0m cnn_params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpre_encoder\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pre_encoder\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder \u001B[38;5;241m=\u001B[39m CNNEncoder(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcnn_params)\n\u001B[0;32m     22\u001B[0m layers \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Build the GNN layers.\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: __init__() got an unexpected keyword argument 'input_spatial_size'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b0e0c549de7e860c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
